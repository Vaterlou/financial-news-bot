from transformers import pipeline
from bs4 import BeautifulSoup
import requests
import time

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
sentiment_pipeline = pipeline("sentiment-analysis", model="blanchefort/rubert-base-cased-sentiment")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# –í–∞—à API –∫–ª—é—á –æ—Ç NewsAPI
NEWS_API_KEY = 'abde548c17e14b56adcd1d8ae20bf083'
TELEGRAM_BOT_TOKEN = '7505061719:AAHjFDzUAoXeJ86myisHUJipHlDPNspJRgk'
TELEGRAM_CHAT_ID = '@russia_market_twits'

KEY_WORDS = [
    '–ö–æ–º–ø–∞–Ω–∏—è', '–ø—Ä–∏–±—ã–ª—å', '—É–±—ã—Ç–æ–∫', '–í—ã—Ä—É—á–∫–∞', '–ê–∫—Ü–∏–∏', '–ö–æ—Ç–∏—Ä–æ–≤–∫–∞', '–†–æ—Å—Ç', '–°–Ω–∏–∂–µ–Ω–∏–µ', '–ü—Ä–æ–≥–Ω–æ–∑', '–û—Ç—á–µ—Ç',
    '–†—ã–Ω–æ–∫', '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–ö–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è', '–†–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å', '–°–¥–µ–ª–∫–∞', '–§–∏–Ω–∞–Ω—Å—ã', '–ë–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ', '–ö—Ä–∏–∑–∏—Å',
    '–î–∏–≤–∏–¥–µ–Ω–¥—ã', '–†–∏—Å–∫–∏', '–°–ª–∏—è–Ω–∏–µ', '–ü–æ–≥–ª–æ—â–µ–Ω–∏–µ', '–†–µ–π—Ç–∏–Ω–≥–∏', '–ö—Ä–µ–¥–∏—Ç', '–î–æ—Ö–æ–¥', '–ü–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–æ', '–†–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ',
    '–°–∞–Ω–∫—Ü–∏–∏', '–ù–∞–ª–æ–≥–∏', '–°—É–±—Å–∏–¥–∏–∏'
]


# –ö–æ–º–ø–∞–Ω–∏–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –±—É–¥–µ–º –∏—Å–∫–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏
COMPANIES = {
    '–ì–∞–∑–ø—Ä–æ–º': 'GAZP',
    '–°–±–µ—Ä–±–∞–Ω–∫': 'SBER',
    '–ú–µ—á–µ–ª': 'MTLR',
    '–û–∑–æ–Ω': 'OZON',
    '–ü–ò–ö': 'PIKK',
    'ALIBABA': 'BABA',
    'JD': 'JD'
}

# URLs –¥–ª—è API
NEWS_URL = 'https://newsapi.org/v2/everything'

def fetch_article_content(url):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ –ø–æ URL.
    """
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            paragraphs = soup.find_all('p')
            article_content = ' '.join([p.get_text() for p in paragraphs])
            return article_content
        else:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {response.status_code}")
            return None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return None


def generate_summary(article_content):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏.
    """
    if article_content:
        summary = summarizer(article_content, max_length=150, min_length=30, do_sample=False)
        return summary[0]['summary_text']
    else:
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏."

def analyze_news(news_description):
    # –ü–æ–¥—Å—á–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –Ω–æ–≤–æ—Å—Ç–∏
    keyword_count = sum(1 for word in KEY_WORDS if word.lower() in news_description.lower())

    if keyword_count == 0:
        return False, None, None

    # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏
    sentiment = sentiment_pipeline(news_description)[0]
    sentiment_label = sentiment['label']
    sentiment_score = sentiment['score']

    # –£—Å–ª–æ–≤–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    if keyword_count > 1 and (sentiment_label == 'NEGATIVE' or sentiment_label == 'POSITIVE'):
        return True, sentiment_label, sentiment_score, keyword_count
    return False, sentiment_label, sentiment_score, keyword_count


def is_news_related_to_company(news_text, company_name, ticker):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏
    if company_name.lower() in news_text.lower():
        return True

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–∏–∫–µ—Ä–∞ –∫–æ–º–ø–∞–Ω–∏–∏
    if ticker.lower() in news_text.lower():
        return True

    return False

def fetch_news(company):
    params = {
        'q': company,
        'qInTitle': company,
        'apiKey': NEWS_API_KEY,
        'language': 'ru',
        'sortBy': 'publishedAt',
        'pageSize': 20,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å
    }
    response = requests.get(NEWS_URL, params=params)
    if response.status_code == 200:
        return response.json()['articles']
    else:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –Ω–æ–≤–æ—Å—Ç–µ–π: {response.status_code}")
        return []

def fetch_stock_price(ticker):
    url = f'https://iss.moex.com/iss/engines/stock/markets/shares/securities/{ticker}.json'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        market_data = data['marketdata']['data'][0]
        last_price = market_data[12]  # –¶–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–¥–µ–ª–∫–∏
        return last_price
    else:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫–æ—Ç–∏—Ä–æ–≤–∫–∏: {response.status_code}")
        return None

def send_telegram_message(message):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    params = {
        'chat_id': TELEGRAM_CHAT_ID,
        'text': message,
        'parse_mode': 'HTML'
    }
    response = requests.get(url, params=params)
    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {response.status_code}")

def main():
    last_published = {}

    while True:
        for company, ticker in COMPANIES.items():
            news_array = fetch_news(company)
            if news_array:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –Ω–æ–≤–æ—Å—Ç–∏
                for news in reversed(news_array):
                    if last_published.get(company) != news['publishedAt'] and is_news_related_to_company(news_text=news['title'], company_name=company, ticker=ticker):

                        # title = news['title']
                        url = news['url']
                        published_at = news['publishedAt']
                        # description = news['description']
                        stock_price = fetch_stock_price(ticker)

                        article_content = fetch_article_content(url)
                        print(article_content)
                        result, sentiment_label, sentiment_score, count = analyze_news(article_content)
                        if not result:
                            continue

                        last_published[company] = published_at
                        description = generate_summary(article_content)

                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è Telegram
                        message = f"<b>{company}</b>\n\n"
                        # message += f"üì∞ {title}\n\n"
                        message += f"üóì <b>–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:</b> {published_at}\n\n"
                        message += f"üí¨ <b>–û–ø–∏—Å–∞–Ω–∏–µ:</b> {description}\n"
                        message += f"üìä <b>–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:</b> {sentiment_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {sentiment_score:.2f})\n"
                        if stock_price:
                            message += f"üí∞ <b>–ö–æ—Ç–∏—Ä–æ–≤–∫–∞ {company}:</b> {stock_price} RUB\n"

                        print(message)
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ Telegram
                        # send_telegram_message(message)
                        # print(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è {company} –≤ Telegram.")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç
        time.sleep(300)

if __name__ == "__main__":
    main()
